# Scaling AI Models with Mixture of Experts (MOE): Design Principles and Real-World Applications
This is the repository for the LinkedIn Learning course `Scaling AI Models with Mixture of Experts (MOE): Design Principles and Real-World Applications`. The full course is available from [LinkedIn Learning][lil-course-url].

![lil-thumbnail-url]

## Course Description

Mixture of Experts (MoE) is a cutting-edge neural network architecture that enables efficient model scaling by routing inputs through a small subset of expert subnetworks. In this course, instructor Vaibhava Lakshmi Ravideshik explores the inner workings of MoE, from its core components to advanced routing strategies like top-k gating. The course balances theoretical understanding with hands-on coding using PyTorch to implement a simplified MoE layer. Along the way, youâ€™ll also get a chance to review real-world applications of MoE in state-of-the-art models like GPT-4 and Mixtral.

## Instructor

Vaibhava Lakshmi Ravideshik

AI Engineer
                            

Check out my other courses on [LinkedIn Learning](https://www.linkedin.com/learning/instructors/).


[0]: # (Replace these placeholder URLs with actual course URLs)

[lil-course-url]: https://www.linkedin.com/learning/instructors/vaibhava-lakshmi-ravideshik?u=104
[lil-thumbnail-url]: https://media.licdn.com/dms/image/v2/D4E0DAQE_IdQLKZzM9A/learning-public-crop_675_1200/B4EZleAjPaKUAY-/0/1758218827345?e=2147483647&v=beta&t=5HgK1heewecpVsp4miork0yxeyYkNXAzLQUeGoTOeUI

