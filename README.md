# Improving your AI Models: Mixture of Experts (MoE)
This is the repository for the LinkedIn Learning course `Improving your AI Models: Mixture of Experts (MoE)`. The full course is available from [LinkedIn Learning][lil-course-url].

![lil-thumbnail-url]

## Course Description
An introductory course on Mixture of Experts covering the basics of MoEs, Mathematical foundations and different architectural variants.


## Installing
1. To use these exercise files, you must have the following installed:
	- [list of requirements for course]
2. Clone this repository into your local machine using the terminal (Mac), CMD (Windows), or a GUI tool like SourceTree.
3. [Course-specific instructions]

## Instructor

Vaibhava Lakshmi Ravideshik

An AI Researcher with ardent passion on Knowledge Graphs, Ontologies, and Multi-modal AI.
     

Check out my other courses on [LinkedIn Learning](https://www.linkedin.com/learning/instructors/).


[0]: # (Replace these placeholder URLs with actual course URLs)

[lil-course-url]: https://www.linkedin.com/learning/
[lil-thumbnail-url]: https://media.licdn.com/dms/image/v2/D4E0DAQG0eDHsyOSqTA/learning-public-crop_675_1200/B4EZVdqqdwHUAY-/0/1741033220778?e=2147483647&v=beta&t=FxUDo6FA8W8CiFROwqfZKL_mzQhYx9loYLfjN-LNjgA

